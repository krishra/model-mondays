Hi everyone. Thanks for joining us for the next episode of Model Mondays. My
0:13
name is Anna. I'm an event planner for Reactor joining you from Redmond, Washington.
0:19
Before we start, I do have some quick housekeeping. Please take a moment to read our code of
0:24
conduct. We seek to provide a respectful environment for both our audience and presenters. While we absolutely
0:31
encourage engagement in the chat, we ask that you please be mindful of your commentary, remain professional and on
0:37
topic. Keep an eye on that chat. We'll be dropping helpful links and checking for
0:42
questions for our presenters and hosts to answer live. Our session is being recorded and will be available to view
0:49
on demand right here on the Microsoft Reactor channel. With that, I'd love to turn it over to
0:55
our host for today. Thanks for joining. Hey everyone, it's a beautiful Monday
1:01
here in Los Angeles, so I hope it's a wonderful Monday wherever you may be in this world. My name is April Gittens and
1:08
I am here to host this episode of Model Mondays. We are going to be taking a
1:13
look at one of my favorite tools that we have here at Microsoft, which is the AI toolkit. But before we get into that,
1:20
Charmela is going to kick us off with the weekly highlights. Chararma, how are you?
1:25
I'm doing great. Can you hear me, April? Yes, I can loud and clear. Yeah, thank you. Um yeah, actually I am
1:33
um caught up in something, but I am doing great. And let's uh get rolling with the highlights. And uh there are
1:39
there's a lot to share this week as well, but it's kind of more on the not the traditional model launch updates.
1:46
It's kind going to be a little bit on like things that we are trying to do as Microsoft on the research side on our
1:53
labs teams or our Microsoft going to be an interesting set of
1:59
updates this week. All right, no problem. All right, so
2:05
with that said, I think we are ready to go ahead and take a look at the weekly highlights.
2:13
Thank you, April, and welcome back to another episode of Model Monday's
2:18
highlights. This week, we are highlighting a set of stories that paint a bold vision of where AI is headed from
2:25
healthcare and coding to S automation and environment. There is a lot to look
2:32
forward to. So, let's dive in. Um, first I want to start off with like
2:39
a must readad. uh we have our Microsoft AI team and a few a few months back uh
2:45
they launched MAI deepseek R1 and now
2:50
they are actually coming back with another model that is in the medical space. So
3:00
this um you have to read this paper. It pro provides a perspective on achieving
3:06
medical super intelligence. And this isn't just a vision paper. It's a blueprint for how large AI models when
3:14
trained safely and responsibly could someday diagnose rare conditions
3:19
accelerate research and even assist in clinical decision making. So it lays out
3:25
the core principles like expert grounding, multimodal understanding and partnership with health care
3:31
professionals as essential aspects to making this real. And then we it also
3:37
talks about the Microsoft AI diagnostic orchestrator, a system that is designed
3:42
to emulate a virtual panel of physicians with di diverse diagnostic approaches to
3:49
collaborate and solve diagnostic cases. And you can expect more updates on this,
3:56
but keep an eye out. Just go check out this u awesome paper and work done by
4:01
the Microsoft AI team. Uh as usual, every highlight will have a QR code. Uh
4:07
so please take a snapshot and that'll take you to the uh to information that you can look at later. And there's also
4:14
um very interesting video that you can watch on this. Um that is um I have linked it in the slides. So if you're
4:20
going back to look at the slides, you can watch the video as well. Moving on,
4:25
next is along the same lines. U this is coming from our Microsoft research team.
4:31
We have project Aurora and it's Microsoft's latest foundational model.
4:37
It's not just for language but for atmospheric science. So it's trained on
4:42
massive volumes of weather data and it can predict variables like temperature and air pollution with high pressure and
4:50
there are spo four specialized variants available for like medium and high resolution weather forecasting, air
4:57
pollution prediction, ocean wave modeling. This is a major leap forward in using AI for climate and
5:04
environmental for forecasting with real world impact on energy disaster response
5:10
and logistics and okay then moving on kind of slightly
5:16
shifting gears we are going into agents. So the last two updates were uh highlights were around the art of poss
5:23
possible things around domain specific models. Now we moving into agents and GitHub announced it's um like at um
5:31
build it announced it next leap forward which is the GitHub copilot agent. I know how many of you I don't know how
5:37
many of you caught that news so I just wanted to bring it up here. Um so this copilot agent uh for GitHub is not just
5:44
autocomplete anymore. It can browse documentation. It can run tests. It can make pull requests and also help you
5:50
refactor. This is a major step forward in turn uh turning copilot into a fully
5:56
capable contextaware developer agent. It's a reminder that copilots are
6:01
getting more autonomous and more useful by the week.
6:07
Moving on um another news on agent side you can uh meet the new S Azure S sur
6:14
agent. So this is Microsoft's answer to LLMdriven operations. Think of it as an
6:20
on call assistant that can triage incidents, generate RCA reports, propose
6:25
mitigations, and even perform safe remediation. It is integrated with Azure
6:30
monitor and DevOps tool for full observability. So, if you're in cloud operations, this is the future of how
6:36
incidents get resolved faster, smarter, and with less manual toil. Again, a
6:42
great paper, a great blog. You can go and take a a read and then there is a video attached to it that'll explain how
6:48
this agent works. Moving on to the final highlight of this week. It's project
6:54
Emily. Amaly, this is kind of um I would say a very early stage private preview
7:01
kind of um a project and I bring it to your attention so that you can um you know sign in for a private preview if
7:07
you're interested or at least keep an eye out for it when it becomes um more full-fledged. Um so project Emily it's
7:14
our first foundry autonomous agent purposebuilt for machine learning engineering task. um you know there's a
7:22
lot of hype with um you know a large language models and like moving on from
7:27
machine learning but there are still there is still a lot of work happening on the core machine learning side and
7:33
this is an agent for that. So now machine learning teams can simply prompt, hey, help me build a model to
7:39
predict customer churn and amily returns like fully validated ML pipelines,
7:45
evaluation metrics, train models, reproducible Python code. I mean, how awesome is that? It actually drastically
7:52
reduces the time it takes to go from idea to working to a working machine learning system. And this is a glimpse
7:59
into how AI can amplify the productivity of machine learning teams and not just
8:04
developers. Um, so yeah, this is a very future looking project. So go take a
8:09
look at it. And that wraps up this week's Model Monday's highlights. Catch
8:14
you next time with more innovative breakthroughs and more model launch and agent launch news. Um, back to you,
8:21
April. All right. Thank you so much, Chararma.
8:27
There's there's a lot of fun great things going on. I'm so glad that you highlighted what we have happening
8:32
around research because a lot of what you tend to see online when people think AI or just like photos generations and
8:39
videos, but there's a lot of great work happening within the research sphere, especially when we think of things like
8:44
productivity, even with the GitHub co-pilot with the agent. I don't know if anyone that's tuning in today happened
8:51
to have checked out the keynote that Satia did where he s he he assigned um an issue to the uh to the co-pilot and
8:59
it actually took action on it. So if you have not tried that out or if you haven't even seen that portion of the
9:05
keynote, I highly suggest checking it out. It is amazing just how much we are
9:11
able to assist or have AI assist us in the ways that we're building. So thank you for highlighting that. I really
9:17
appreciate it. Yeah, I mean I I I was pretty much excited about all the announcements we
9:23
did at Build and some of the news though it was announced at Build, it got I mean
9:28
um it's good to reh highlight it because there was a lot happening at Build and you know it's good to put a spotlight on
9:35
something that would be interesting to this community and um yeah that was one main reason why I wanted to bring it
9:40
back. Oh, well, thank you. And speaking of build and announcements, that does
9:47
bring me to the um the highlight section or the the tool section that we're going to discuss here today. So, with that
9:54
said, for our weekly spotlight, for those who don't know, and I don't think I even shared it in the beginning. I was
10:00
so excited to start. My name is April Gins. I'm a principal cloud advocate here at Microsoft within our AI team.
10:07
And my day-to-day tasks um they very pretty much vary. I would say but the
10:12
one area that I've had a lot of focus on in our current time has been the AI
10:17
toolkit and I've been working with a very wonderful team who are the creators and builders and engineers and maintain
10:24
maintainers and PMs behind this toolkit. So lucky for us all um Leo Yao is
10:31
joining us today and he is the PM for um for uh some of our tools that we have
10:37
available there. So, I'm super excited to have him come speak with us at Build.
10:42
There were various sessions where the AI toolkit as well as some of the other toolkits within the Windows AI or the
10:49
Microsoft AI toolkit um were shared. And so, Leo is going to join us today, as
10:56
you can see here, to actually provide us with a demo of how all these tools come together. Leo, I'm not going to steal
11:02
your spotlight any longer. So, hello. How are you? Thanks for joining us. Yeah, I'm doing great. Thanks, April.
11:08
Yeah. Hello everyone. My name is Leo and I am a product manager in DevD at Microsoft. So, some of the tools um I
11:16
help build is the Azure AI Foundry extension in VS Code and also help a little bit on the AI toolkit side of
11:22
things as well. So, yeah, excited to show everyone uh what are we uh doing in
11:28
the latest updates and even provide some potential sneak peeks to some feature
11:33
updates. Perfect. All right. So then without further ado, let's take a look at that
11:40
demo.
11:48
Hello everyone and welcome to our live stream. Today we're are going to dive into the exciting world of developing an
11:55
AI agent using VS Code. My name is Leo and I'm thrilled to guide you through
12:00
this process. In the extensions tab, search for AI tools pack which is a curated set of
12:08
essential extensions for building generative AI applications and agents in
12:13
Visual Studio Code. It includes AI toolkit which has a comprehensive set of
12:19
tools for exploring and creating AI agents with local and remote generative
12:24
AI models. Azure AI Foundry extension which provides AI models and agent
12:30
management and deployment capabilities using the Azure AI foundry service and
12:36
various other development tools such as GitHub copilot data wrangler to help you
12:41
with code and visualization. The first thing that we need to develop
12:46
an AI agent is a language model. In VS Code sidebar, click on the AI toolkit
12:52
icon and open its sidebar. Under the models, click on catalog. This will open
12:58
a set of models curated by the AI toolkit team. At the top, you can see a
13:03
few popular models. Right now, we have DPCR1, 54, and GPT4.1.
13:10
Under that section, we split this model catalog into different model hosts, including GitHub, Azure AI, Foundry,
13:18
local models with Olama and Onyx, and other models. If you do not know which model to you're
13:24
looking for, you can click on each model to learn more about it. If you have a specific model in mind, you can use the
13:31
search and filter functionality to find the exact model. We can test out some models by using
13:37
GitHub deployment option. Under the GitHub models, I can click on add model for a GPT4 model on the left. The GPD4
13:46
model now appears under the model section along with a few other models I
13:52
have already deployed under the local model section. You can see I have a Java
13:57
3 model through the O lama and I have access it in AI toolkit. Back to the
14:02
GPT4 model. Right clicking on it gives me an option to open it in a model
14:08
playground. In here I can enter a conversation with the model to try out its capabilities. You can also change
14:15
some of the settings associated with the models on the right. For example, I can ask it to explain recursion to someone
14:22
without programming knowledge.
14:29
And after waiting a while, we see that a response from the model
14:35
has came back with a short and concise description of what recursion is.
14:41
Since this model we're using is a GitHub model and it is available free for everyone to use, it has a pretty low
14:48
rate limit that might not be suited for our agent development scenarios. So
14:54
before I start with the agents, I will deploy a model to Azure AI Foundry with a higher rate limit so I don't hit the
15:02
token limit. Clicking on the Azure AI Foundry icon, the sidebar can bring you to the Azure
15:08
AI Foundry tab. From here I can create a new Azure AI foundry project. If you
15:13
already have an AI foundry project, you can choose to select the existing one. And since I'm creating a new one and I
15:19
am already signed in with my Azure account, all I need to do is provide a project name.
15:26
Then I will create a new resource group and the project will be set up. Since
15:32
the project creation will take some time, we'll skip forward until it is created.
15:39
Okay, now the project is finally created. You can see it under the resource section with models, agents,
15:46
and threads. Going back to the model catalog, I can filter it now to the Azure AI Foundry host. Then I can deploy
15:54
the same GPT4 model and deploy it to Azure AI Foundry. This time in the model
16:01
deployment page here you can see many of these details are already pre-filled based on the Azure AI project that I've
16:08
configured. So the only need thing I need to change is desire token limit and
16:13
I'm going to set a pretty high number so we don't run into any issues there. Now
16:18
after it finishes deploying you can see it under the Azure AI foundry section of models and going back to the Azure AI
16:25
foundry section of VS code we can see the model details here as well.
16:31
Now we can start building the agent. Under tools there is an agent builder tool. It will open the agent builder UI.
16:38
From there I can decide what kind of agent to make. For models I will pick the newly created GPD40 model that I
16:45
have on Azure AI foundry. For system prompt, I can use this cool feature called generate to to ask it to generate
16:53
a system prompt based on what I want the agent to do. For example, I want to make an agent that acts as a tutor that helps
17:00
students with algebra questions. So, I can type in what I want and ask it to generate the system prompt.
17:09
Next up is a user prompt, which is a question that I want to ask the agent. For this, I will ask it to answer a
17:17
pretty common systems of equations problem that you might see in school. Pressing on the run button, we can see
17:24
what the agent responses on the right along with the correct answers. And for the sanity check, by go doing the math
17:31
ourselves, we do see that the answer is indeed correct. Let's take a look at a
17:37
more complex scenario for agents. Here, I already have an agent that's assigned
17:42
to be a helper agent for escape room puzzles. It will provide responses to the
17:48
player's questions, provide hints, and help troubleshoot should any problems arise. It will also speak in a tone that
17:55
is similar to the room's theme. For this agent, I have configured it so
18:00
that the agent has access to an MCP server where it has a set of functions that the agent can call to help it
18:06
answer the user's questions. Using the fast MCP server package in Python, I can
18:11
define tools by using the atmc.tools tag on the function definitions. To add
18:17
it to the agent, clicking on the plus icon allows me to choose the MCP server as an option. From there we can add a
18:25
new server and connecting to a to an existing one that we have that I shown
18:30
earlier. Since this file is on my local machine, I will choose the with a standard IO. Entering the correct Python
18:37
command to run that file. Oh, and the input box gives me the MCP server and an
18:43
ID. And then the MCC server is available for me to for the agent to use. Once
18:49
you're satisfied with the agent that you built, you can view the code needed to run the agent. Since this model is on
18:55
Azure is on Azure, we can use the Azure AI inference SDK. In this Python file, it use the API key
19:03
to authenticate us. You can get the API key under the AI foundry tab under models. Right click on the model and
19:10
copy API key. You can also see it in the model or card itself.
19:16
I can set that as my environment variable and run the file and we will see the result of that run.
19:25
Now we have an agent working in our local environment. We will also deploy one to Azure AI Foundry. Navigate to the
19:32
Azure AI Foundry tab. Under the agent section, clicking on the plus icon will
19:37
open up the agent designer and YAML file for the agent side by side. The ID field
19:43
will be automatically generated once we upload it to Azure AI foundry. So the
19:48
next thing that we need to do is give it a name.
19:56
The language model that I will use and I am going to use the same one that we deployed earlier.
20:08
And we can also configure any additional tools that the agent will be able to use. For now, the tools that's available
20:15
are Bing search, file search, code interpreter, and open API tools. More
20:20
tools such as MTC servers, connected agents will be available in the future.
20:26
For now, I will configure this agent to have Bing search tool and a code interpreter tool.
20:32
In the YAML file, we have IntelliSense enabled to allow you to quickly select the correct resources to use. For
20:39
example, for models, it will show you a list of models that you have deployed in
20:44
your current project as shown earlier. And for Bing search, we will show you a
20:49
list of all the Bing search resources that you have access to as well as any templates for existing and supported
20:56
tools. In the future, we will also have dedicated UIs to allow you to configure
21:02
these tools. We will use the instructions for the tutor agent earlier.
21:09
Once we are done configuring the agent, we can create it on Azure AI Foundry. Under the agent section, you can see a
21:16
list of agents that you have deployed. Let's take a look at the shopping agent that I have created earlier.
21:24
Clicking on it will open up the view only version of the agent designer where you can see agent details. You can also
21:32
click on edit agent to bring back up the agent editing mode for the designer
21:38
file. You can also view the agent code file to run it with your own code.
21:46
The shopping agent has two main capabilities. Provide product recommendations and suggesting an
21:51
outfit. Now let's see what the this agent is capable of by using the agent playground.
21:59
Since this agent is preconfigured with the bing grounding tool, I can ask a question such as can you suggest a cha
22:06
casual outfit for me for tomorrow in Seattle.
22:14
After waiting for a while, the agent will come back with the
22:19
response. It's looking for some information about the weather, about my wardrobe and the activity and it did a
22:26
bing grounding search to find out about the temperature and give me some details. And I ask you to clarify some
22:33
details about what kind of items do I want. So I will provide with additional information
22:39
asking it to give me a few options and then it provided with three
22:46
different options on what I could wear tomorrow. Another thing I can do to ask the agent
22:52
is to ask it to suggest me some running shoes for me to wear to go along with my
22:57
outfit since I want to find out what kind of options are there that the agent
23:02
is able to find for me.
23:11
So after I use another banking grounding search tool, it found three different options that I suggested to me along
23:18
with their price range and a link to purchase them if I wish to do so.
23:24
If you want to look back on any of the conversations that you had with your agent, you can open the threads view on
23:30
the left hand bar and pick out the right thread ID that you see that you have
23:35
currently. Clicking on it will open up the history view of your conversation with that agent.
23:43
So you can click onto any of the individual steps to view the step details. And this and this one it shows
23:50
me all also the tool calls that they use as well as well as any of the tokens
23:56
that they used. To recap what we just went through in a demo, we installed the AI tools
24:03
extension pack which includes AI toolkit, Azure AI foundry, data wrangler, and GitHub copilot to help us
24:10
develop an AI agent. We used the AI toolkits model catalog to browse and
24:16
deploy a GitHub model. Then tested it out in the model playground. After that,
24:21
we developed another model to Azure AI foundry that is dedicated to agents.
24:27
We use AI toolkits agent builder to configure a local agent with MCP server
24:33
and ran the agent with our own Python code. We switched over to Azure AI foundry to deploy an agent with bing
24:40
grounding and code interpreter tool and tested an already deployed agent in
24:45
agent playground as well as our own conversation with the agent in the threads view. These are just a few
24:52
features that are available in both AI toolkit and Azure AF foundry extension.
24:57
To discover the full feature set, please download the extension pack and try it out.
25:06
All right, that was a really great demo. Thank you so much, Leo. I really
25:12
appreciate it. Keep an eye on the chat here. We did get a question from Kevin. Kevin was curious to know how do you
25:19
know when like the MCP server is disconnected or have stopped.
25:25
Yeah. So basically uh once you configure an MCP server into AI toolkit it's it is
25:31
not running until you basically ask the question to the agent right so uh once
25:37
you type that question in and press run and that's where the agent will start the MCP servers themselves and they will
25:44
it will run the write commands to the MCP server retrieve the answers and
25:50
return back the answer. It will stop once the basically once the agent has
25:55
finished running. Okay. All right. Got it. Thank you so much, Kevin. I hope that that answered
26:00
your question. Following up on the MCP topic before we come to a close. So,
26:05
it's great that through um through the through the extensions, we are able to connect to an MTP server and I did see
26:12
that we connected for one for the escape room. Is it possible to connect to more than one?
26:17
Yeah, definitely. So uh I think as part of the experience I was showing in the demo uh when you click on the plus
26:24
button you can ch uh it only I only had one configured but you can easily just
26:29
click on the plus uh button again add a second one it probably will have a different um Python file or just any MCP
26:37
file or even a remote one but yeah it will just be added to the list of tools that agent has.
26:44
All right, cool. One additional question to that. Circling back to the models
26:49
portion when we were exploring models in the catalog. What happens if someone has their own model? Can they bring their
26:55
own model to the extension as well and use that as they're creating an agent? Yeah. Uh that's definitely possible. I
27:01
think uh I went over briefly in the demo where uh there was like a Gemma 3 model
27:07
under a local model section. So if you have a local model you can uh just use
27:12
whether you use Ola or Onyx you can easily add it as well. Uh if you have a remote model host it somewhere else. You
27:18
all you need I think is just a URL to the model and any authentication method.
27:24
Uh for example if you're using API keys you can paste the API keys will hash it for you and it will just be it will just
27:31
be available right there. All right cool. one final question and then I'll bring us to a close because
27:38
there's so many questions that I know will come up once we get to the AMA this Friday. But my question is based on what
27:45
we've seen in the demo once someone's ready to actually migrate to doing everything just purely with code um the
27:52
extension does the extension generate code for people to move forward with doing things programmatically?
27:58
Yeah. So there are uh in both AI toolkit and Azure AI foundry there's there's
28:04
always a generate code button because we know that developers in VS code love
28:09
working with code first. So we always want to provide you with that option and
28:14
once you do get get it you should be able to get it running pretty much without any additional configuration.
28:21
You might need to like set an environment variable or two or sign into your Azure account if you haven't
28:27
already done so, but it should be uh just one or two additional things and
28:33
then you're ready to go. All right, awesome. Thank you so much for that, Leo. Um folks that are tuning
28:40
in, if you haven't already tried the AI toolkit or if you have tried the AI
28:46
toolkit, um definitely give it a try. You'll probably have some questions that'll come up as you're trying things
28:52
out, which is perfect because this upcoming Friday, we will have an AMA within the Discord. And I'll share more
28:58
on the details on that in a bit. But um before I bring us to a close, I just
29:03
want to say thank you so much, Leo, for joining us today and sharing this demo.
29:09
Um any closing words around the um extensions by chance? Yeah. Uh first of all, thank you, April,
29:15
for inviting me to join this. It was great to show what my what my team and everyone has been working on for the
29:22
past few months. So yeah, everyone please go out and try both extensions or
29:29
try the extension pack and if you have any feedback feel free to uh we we have
29:34
GitHub we have GitHub repos for both. So leave a comment, leave an issue or
29:39
submit a bug report if you do find any. We're happy to take a look and yeah, your feedback is what drive us to
29:45
improve. All right, perfect. Okay, so just want to share with you all
29:52
what's coming up next week. We have next week's session or stream is going to be
29:57
on fine-tuning and distillation. It'll be hosted by Lee who I know is hanging out in the chat with you all today and
30:04
then Dave will be joining the stream as well from Core AI team to um to share
30:10
more about what's happening around fine-tuning and distillation. In addition to that, sorry, there we go.
30:16
Friday, I've mentioned it. We have an AMA. If this is your first time tuning in, welcome, of course. But just as a
30:23
heads up, on Fridays, we are going to do an AMA related to this topic. If you're
30:28
not already part of the Discord server, I beg you to go join. That's where you
30:34
can have or that's where you will find a lot of conversations that are happening around um various various aspects of
30:41
creating agents and using AI. And so this Friday within the discord server we
30:47
will have an AMA focused on the extensions that Leo shared with us today
30:52
in the demo. So do bring your questions. Information is here on the slide. That's going to be 1:30 to 2:00 p.m. Eastern
31:00
this Friday, July 11th. And then also we have some additional information and
31:06
links here for you all in case you want to check out any of the um upcoming
31:11
episodes for Model Mondays. We have a QR code here on the screen as well as a link so that way you can RSVP to
31:18
upcoming episodes. You'll see me again for a couple of future episodes as well.
31:23
Then we also have the AMA as I mentioned. So if you didn't catch the link before, that link is here again on
31:28
the screen. So do come join us. We look forward to having you there. And then also we have recaps as well um um on a
31:37
forum as well. So that way you can get a recap of what we shared during model Mondays. I know that Nitia Lee has been
31:45
working on putting links in the chat. So definitely do scroll through and find those links and we have them here as
31:51
well. If you were curious as to whether you'll be able to rewatch everything we went through today, especially the
31:57
demos, those are also going to be linked as well um throughout the chat. I know Nitia recently just shared a speaker
32:03
deck for example. But with that said, thank you all for joining on this very
32:09
beautiful Monday. I hope that wherever you are, whether it's the morning, the afternoon, or the night time, that you
32:15
have a really wonderful Monday. Um, and I look forward to hopefully seeing all
32:20
of you plus more at this upcoming AMA. So, with that said, thank you all for
32:25
joining and take care.
33:01
Hey, hey, hey.