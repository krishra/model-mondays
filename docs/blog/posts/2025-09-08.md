---
date:
    created: 2025-09-08
draft: false
authors: 
  - sharda
  - nitya
categories:
  - Recaps
  - Season-02
tags:
  - observability, evaluations, red-teaming, genaiops
---

# S2:E12 Understanding Models & Observability with Minsoo Thigpen

> Model Mondays is a weekly series to build your Azure AI Foundry Model IQ. In this episode, we focus on Models & Observability — and learn how observability is about instrumenting AI apps to collect signals that provide insight into model behavior & performance.

_This week in Model Mondays, we focus on Models & Observability — and learn how observability is about instrumenting AI apps to collect signals that provide insight into model behavior & performance across the GenAIOps lifecycle. Read on for my recap of Minsoo Thigpen's insights on evaluations, red-teaming & end-to-end observability in Azure AI Foundry for trustworthy AI!_

![Card](./../../season-02/img/S2-E12.png)

<br/>

### About Model Mondays

Model Mondays is a weekly series designed to help you build your Azure AI Foundry Model IQ step by step. Here's how it works:

- 5-Minute Highlights – Quick news and updates about Azure AI models and tools on Monday
- 15-Minute Spotlight – Deep dive into a key model, protocol, or feature on Monday
- 30-Minute AMA on Friday – Live Q&A with subject matter experts from Monday livestream

If you want to grow your skills with the latest in AI model development, [Model Mondays](https://aka.ms/model-mondays) is the place to start. Want to follow along?

- [Register Here](https://developer.microsoft.com/en-us/reactor/series/S-1485/?wt.mc_id=studentamb_263805) - to watch upcoming Model Monday livestreams 
- [Watch Playlists](https://aka.ms/model-mondays/playlist) to replay past Model Monday episodes 
- [Register Here](https://discord.gg/azureaifoundry?event=1382864811649536120?wt.mc_id=studentamb_263805) - to join the AMA on Models & Observability on Friday Sep 12
- [Visit The Forum](https://aka.ms/model-mondays/forum?wt.mc_id=studentamb_263805) - to view Foundry Friday AMAs and recaps

<br/>

## Spotlight On: Models & Observability

### 1. What is this topic and why is it important?

Models & Observability focuses on instrumenting AI applications to collect meaningful signals about model behavior, performance, and reliability across the entire GenAIOps lifecycle. This includes systematic evaluation frameworks, red-teaming for security assessment, and comprehensive monitoring that provides insights into how models perform in real-world conditions.

This is crucial for building trustworthy AI systems that can be monitored, debugged, and improved over time. Without proper observability, AI applications become black boxes where issues are difficult to detect and resolve, leading to poor user experiences and potential safety concerns.

### 2. What is one key takeaway from the episode?

The key insight is that observability for AI systems requires a fundamentally different approach than traditional software monitoring. AI models can fail in subtle ways that traditional metrics might miss, and their behavior can drift over time. Comprehensive observability includes not just performance metrics, but also quality assessments, fairness evaluations, and security monitoring through techniques like red-teaming.

### 3. How can I get started?

To get started with Models & Observability:
1. Implement basic logging and monitoring for your AI applications
2. Set up evaluation frameworks to systematically assess model performance
3. Explore red-teaming techniques to identify potential vulnerabilities
4. Use Azure AI Foundry's built-in observability tools and dashboards
5. Establish baselines and alerts for key quality and performance metrics

### 4. What's new in Azure AI Foundry?

[This section will be filled in later]

<br/>

## My A-Ha Moment

My biggest realization was understanding that monitoring AI applications is fundamentally different from monitoring traditional software. I used to think that tracking basic metrics like response time and error rates would be sufficient, but Minsoo's presentation opened my eyes to the complexity of AI observability.

The concept that really struck me was "model drift" — the idea that an AI model's performance can degrade over time even if the code hasn't changed, simply because the real-world data it encounters shifts from what it was trained on. This means we need continuous evaluation and monitoring, not just deployment-time testing.

The red-teaming approach was particularly eye-opening. I hadn't fully appreciated how systematically trying to break or misuse AI systems is essential for building trustworthy applications. It's like having a security team that's constantly trying to find vulnerabilities, but specifically focused on AI-related risks like bias, hallucinations, and misuse.

<br/>

## Coming up Next Week

Next week, we wrap up Season 2 with Open Source Models. We'll explore the Hugging Face collection on Azure AI Foundry Models, which currently has over 10K options. Learn when to use them, how to select the right one, and how to get started with Hugging Face models on Azure AI — plus how you can request new additions. [Register Here!](https://aka.ms/model-mondays/rsvp)

![Open Source Models](./../../season-02/img/S2-E13.png)

<br/>

## Join The Community
Great devs don't build alone! In a fast-paced developer ecosystem, there's no time to hunt for help. That's why we have the Azure AI Developer Community. Join us today and let's journey together!

1. [Join the Discord](https://discord.com/invite/QR3kaErCRx?wt.mc_id=studentamb_263805) - for real-time chats, events & learning
2. [Explore the Forum](https://aka.ms/model-mondays/forum?wt.mc_id=studentamb_263805) - for AMA recaps, Q&A, and help!

<br/>
 
## About Me:
I'm Sharda, a Gold Microsoft Learn Student Ambassador interested in cloud and AI. Find me on Github, Dev.to, Tech Community and Linkedin. In this blog series I have summarized my takeaways from this week's Model Mondays livestream.
