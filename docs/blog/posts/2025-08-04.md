---
date:
    created: 2025-08-04
draft: false
authors: 
  - sharda
  - nitya
categories:
  - Recaps
  - Season-02
tags:
  - on-device, local-ai, onnx-runtime, edge-computing
---

# S2:E8 Understanding On-Device & Local AI with Maanav Dalal

> Model Mondays is a weekly series to build your Azure AI Foundry Model IQ. In this episode, we focus on On-Device & Local AI — and learn how on-device inference is critical for running AI models locally on your own hardware for edge computing needs.

_This week in Model Mondays, we focus on On-Device & Local AI — and learn how on-device inference is critical if you want to run AI models locally, on your own hardware, especially for edge computing needs. Read on for my recap of Maanav Dalal's insights on Foundry Local — a solution built on ONNX Runtime (for use in CPUs, NPUs & GPUs) that takes you from prototype to production._

![Card](./../../season-02/img/S2-E8.png)

<br/>

### About Model Mondays

Model Mondays is a weekly series designed to help you build your Azure AI Foundry Model IQ step by step. Here's how it works:

- 5-Minute Highlights – Quick news and updates about Azure AI models and tools on Monday
- 15-Minute Spotlight – Deep dive into a key model, protocol, or feature on Monday
- 30-Minute AMA on Friday – Live Q&A with subject matter experts from Monday livestream

If you want to grow your skills with the latest in AI model development, [Model Mondays](https://aka.ms/model-mondays) is the place to start. Want to follow along?

- [Register Here](https://developer.microsoft.com/en-us/reactor/series/S-1485/?wt.mc_id=studentamb_263805) - to watch upcoming Model Monday livestreams 
- [Watch Playlists](https://aka.ms/model-mondays/playlist) to replay past Model Monday episodes 
- [Register Here](https://discord.gg/azureaifoundry?event=1382863345777901670?wt.mc_id=studentamb_263805) - to join the AMA on On-Device & Local AI on Friday Aug 08
- [Visit The Forum](https://aka.ms/model-mondays/forum?wt.mc_id=studentamb_263805) - to view Foundry Friday AMAs and recaps

<br/>

## Spotlight On: On-Device & Local AI

### 1. What is this topic and why is it important?

On-Device & Local AI refers to running AI models directly on local hardware rather than relying on cloud-based inference. This includes everything from edge devices and mobile phones to local servers and workstations. Foundry Local, built on ONNX Runtime, enables developers to deploy AI models across different hardware configurations (CPUs, NPUs, and GPUs) while maintaining the same development experience from prototype to production.

This is crucial for scenarios requiring low latency, data privacy, offline operation, or compliance with data sovereignty requirements. It enables AI capabilities in environments where cloud connectivity is unreliable or where sensitive data cannot leave the premises.

### 2. What is one key takeaway from the episode?

The key insight is that local AI deployment doesn't have to mean compromising on development experience or model performance. Foundry Local bridges the gap between cloud development convenience and local deployment requirements, allowing developers to build and test in the cloud while seamlessly deploying to edge environments with optimized performance across different hardware configurations.

### 3. How can I get started?

To get started with On-Device & Local AI:
1. Explore Foundry Local and ONNX Runtime capabilities
2. Identify your specific edge computing requirements (latency, privacy, offline needs)
3. Convert your models to ONNX format for optimal local performance
4. Test deployment across different hardware configurations (CPU, NPU, GPU)
5. Optimize models for your target hardware using ONNX Runtime tools

### 4. What's new in Azure AI Foundry?

[This section will be filled in later]

<br/>

## My A-Ha Moment

My biggest realization was understanding that "local AI" isn't just about disconnected or offline scenarios — it's about choosing the right deployment strategy for each use case. I used to think that local deployment was only for situations where you couldn't use the cloud, like remote locations or high-security environments.

But Maanav's presentation showed me that local AI can often provide better user experiences even when cloud connectivity is available. The immediate response times, reduced data transfer costs, and enhanced privacy can make local deployment the preferred choice for many applications.

What really clicked for me was seeing how Foundry Local maintains the same development workflow I'm used to in the cloud while giving me the flexibility to deploy wherever makes the most sense. It's not about choosing between cloud and local — it's about having the freedom to deploy optimally for each situation.

<br/>

## Coming up Next Week

Next week, we dive into Models for AI Agents with Mona Whalin. We'll explore how to build agentic AI applications and learn about resources for configuration and design patterns. We'll discuss the Azure AI Foundry Agent Catalog — open-source samples you can explore to accelerate your agent development by integration into projects! [Register Here!](https://developer.microsoft.com/en-us/reactor/events/26128/)

![Models for AI Agents](./../../season-02/img/S2-E9.png)

<br/>

## Join The Community
Great devs don't build alone! In a fast-paced developer ecosystem, there's no time to hunt for help. That's why we have the Azure AI Developer Community. Join us today and let's journey together!

1. [Join the Discord](https://discord.com/invite/QR3kaErCRx?wt.mc_id=studentamb_263805) - for real-time chats, events & learning
2. [Explore the Forum](https://aka.ms/model-mondays/forum?wt.mc_id=studentamb_263805) - for AMA recaps, Q&A, and help!

<br/>
 
## About Me:
I'm Sharda, a Gold Microsoft Learn Student Ambassador interested in cloud and AI. Find me on Github, Dev.to, Tech Community and Linkedin. In this blog series I have summarized my takeaways from this week's Model Mondays livestream.
