---
title: Model Mondays S2E03 - AMA on SLMs and Reasoning
published: false
description: Join us for the #FoundryFriday AMA with Mojan Javaheripi on Jul 3 to talk about SLMs and Reasoning - https://aka.ms/model-mondays/discord
tags: beginners, azureaifoundry, ama, modelmondays
series: Model Mondays Season 2
cover_image: https://github.com/microsoft/model-mondays/blob/main/docs/season-02/img/S2-E3-AMA.png?raw=true
---

![Model Mondays Banner](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ifnd5j9v3lbqzkctlvx1.png)

Model Mondays is a weekly series that helps you come up to speed with the fast-moving world of AI models. Here are 3 actions you can take to build your model IQ:

1. **Watch** [Model Mondays Live](https://aka.ms/model-mondays/live) - for news roundup & tech spotlights in 30 mins
2. **Join** [Foundry Friday AMA](https://aka.ms/model-mondays/forum) - for discussions with Q&A featuring AI experts
3. **Subscribe** [Model Mondays Newsletter](https://aka.ms/model-mondays/newsletter) - a weekly pulse on AI innovation & tech

---

## Spotlight On: SLMs and Reasoning

This week, we shine the spotlight on **SLMs and Reasoning** with Mojan Javaheripi - a Senior Researcher on the Microsoft Research team behind the Phi-4 reasoning models. We explore how Phi-4 reasoning models are redefining small language models (SLMs) for the agentic era of apps, especially on resource-constrained devices.

- 👉🏽 [Register for the AMA](https://discord.gg/azureaifoundry?event=1382861149288005693) on our Azure AI Foundry Discord
- 👉🏽 [Post Your Questions](https://github.com/orgs/azure-ai-foundry/discussions/76) on our Discussion Forum

Check out the slides from the presentation for more details!

{% speakerdeck cefcc8825d3c48fca1beb91a02cfd76d %}

---

## Phi-4 Reasoning: Power Through Precision

In her spotlight talk, Mojan introduced the Phi-4-reasoning model and a Phi-4-reasoning-plus model and walked us through the techniques used to create these variants in a manner that allows them to work in resource-constrained environments, while providing comparable accuracy to larger language models.

### 1. Watch The Recording

Watch the replay from the livestream first, to get a sense for what the talk covered. The slides for the talk are linked here: [Download The Slides](./../../../labs/season-02/pdf/S2-E03-SLMs-Reasoning.pdf)

{% youtube VLQKZq8L9Uk %}

### 2. AI-Generated Summary

> The following summary was generated using GitHub Copilot in Visual Studio Code (Agent Mode) with the _Claude Sonnet 4_ model. The assigned task was to `Analyze the S2-E03-SLMs-Reasoning PDF and summarize it in 1-2 paragraphs. End with 3-5 bullet points as takeaways about Phi-4 Reasoning` - the model installed required PDF reader tooling and generated this summary, which was manually reviewed for accuracy.

The "S2-E03-SLMs-Reasoning" presentation focuses on Microsoft's Phi-4 Reasoning models, which demonstrate that small language models (SLMs) can achieve exceptional reasoning capabilities through strategic training approaches. The presentation introduces two variants: Phi-4-reasoning (efficiency-optimized) and Phi-4-reasoning-plus (accuracy-maximized), both built on a 14B parameter architecture that competes with models 5-50 times larger. The models were trained using a comprehensive approach combining supervised fine-tuning (SFT) on 1.4M+ curated STEM and coding prompts, followed by reinforcement learning optimization. The training methodology emphasizes data quality over quantity, with careful curation of teachable prompts at the boundary of base model capabilities.

The presentation showcases impressive benchmark results across mathematical reasoning (78% on AIME 2025), scientific reasoning (69.3% on GPQA Diamond), and coding tasks (53.8% on LiveCodeBench). These models demonstrate strong performance on complex reasoning tasks while maintaining efficiency, making them suitable for deployment on modest hardware including tablets and phones. The presentation positions these models as game-changers for applications requiring step-by-step reasoning, from intelligent tutoring systems to autonomous coding agents, proving that strategic training can overcome the traditional scaling paradigm in AI development.

### 3. **Key Takeaways**

> These are the takeaways generated by the prompt above

• **Efficiency Through Strategic Training:** Phi-4 reasoning models achieve competitive performance with models 5-50x larger through careful data curation, synthetic training data, and reinforcement learning rather than brute-force scaling

• **Dual Strategy Approach:** The two-model strategy offers users a choice between Phi-4-reasoning for efficiency (faster inference, lower cost) and Phi-4-reasoning-plus for maximum accuracy (1.5x longer reasoning traces, higher precision)

• **Real-World Impact:** These models are already being deployed in production applications like autonomous coding agents (RooCode, Cline) and demonstrate significant improvements in mathematical reasoning (+40 percentage points), coding (+25 percentage points), and planning tasks (+30-60 percentage points) compared to base models

---

<br/>

## Build Your Model IQ!

Model Mondays Season 2 is currently scheduled to go from June to September, covering the 12 key topics shown below.

- 👉🏽👉🏽 [Register for Upcoming Livestreams](https://aka.ms/model-mondays/rsvp) to get reminders
- 👉🏽👉🏽 [Register for Upcoming AMAs](https://github.com/orgs/azure-ai-foundry/discussions/54) to get reminders

![Season 2](https://github.com/microsoft/model-mondays/blob/main/docs/season-02/img/S2-Agenda.png?raw=true)

<br/>

## This Week In Model Mondays

Want an easy way to catch up on all the news? Check out **This Week in Model Mondays** - a recap written by Sharda Kaur - [every week, on the Educator Tech Community Blog](https://aka.ms/faculty)

![Sharda Banner](https://github.com/microsoft/model-mondays/blob/main/docs/season-02/img/people/S2-Sharda.png?raw=true)

You can also follow Sharda right here on dev.to

{% user sharda_kaur %}
